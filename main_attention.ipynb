{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c508c51",
   "metadata": {},
   "source": [
    "## Cài đặt môi trường"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2173cf",
   "metadata": {},
   "source": [
    "### Import thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25df0663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5038cd9f",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54e3fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 64\n",
    "EMBED_DIM = 512\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "N_EPOCHS = 20\n",
    "CLIP = 1.0\n",
    "TEACHER_FORCING_RATIO = 0.5\n",
    "MAX_LEN_DECODING = 50\n",
    "CHECKPOINT_DIR = './checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cfa017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.1+cu117\n",
      "CUDA version: 11.7\n",
      "Is CUDA available: True\n",
      "GPU device name: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU device name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a09bb11",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05a71028",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f57ef",
   "metadata": {},
   "source": [
    "### Xây dựng từ điển"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af832272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(file_path, tokenizer):\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield tokenizer(line.strip().lower())\n",
    "\n",
    "def build_vocab(file_path, tokenizer, max_tokens=10000):\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        yield_tokens(file_path, tokenizer),\n",
    "        specials=['<unk>', '<pad>', '<sos>', '<eos>'],\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    return vocab\n",
    "\n",
    "# adjust data paths if needed\n",
    "EN_TRAIN = './data/train.en'\n",
    "FR_TRAIN = './data/train.fr'\n",
    "EN_VAL = './data/val.en'\n",
    "FR_VAL = './data/val.fr'\n",
    "\n",
    "en_vocab = build_vocab(EN_TRAIN, en_tokenizer)\n",
    "fr_vocab = build_vocab(FR_TRAIN, fr_tokenizer)\n",
    "\n",
    "PAD_IDX_EN = en_vocab['<pad>']\n",
    "PAD_IDX_FR = fr_vocab['<pad>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f644f8f",
   "metadata": {},
   "source": [
    "### Encode sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60e404ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentence, tokenizer, vocab):\n",
    "    tokens = tokenizer(sentence.strip().lower())\n",
    "    ids = [vocab['<sos>']] + [vocab[token] for token in tokens] + [vocab['<eos>']]\n",
    "    return torch.tensor(ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e288010",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7efb9044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_file, trg_file, src_tokenizer, trg_tokenizer, src_vocab, trg_vocab):\n",
    "        with open(src_file, encoding='utf-8') as f:\n",
    "            self.src_lines = [l.strip() for l in f.readlines()]\n",
    "        with open(trg_file, encoding='utf-8') as f:\n",
    "            self.trg_lines = [l.strip() for l in f.readlines()]\n",
    "        assert len(self.src_lines) == len(self.trg_lines)\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = encode_sentence(self.src_lines[idx], self.src_tokenizer, self.src_vocab)\n",
    "        trg = encode_sentence(self.trg_lines[idx], self.trg_tokenizer, self.trg_vocab)\n",
    "        return src, trg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef32fa9",
   "metadata": {},
   "source": [
    "### Collate fn với Padding & Packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6cb6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_lengths = torch.tensor([len(s) for s in src_batch], dtype=torch.long)\n",
    "    trg_lengths = torch.tensor([len(t) for t in trg_batch], dtype=torch.long)\n",
    "\n",
    "    sorted_idx = torch.argsort(src_lengths, descending=True)\n",
    "    src_batch = [src_batch[i] for i in sorted_idx]\n",
    "    trg_batch = [trg_batch[i] for i in sorted_idx]\n",
    "    src_lengths = src_lengths[sorted_idx]\n",
    "    trg_lengths = trg_lengths[sorted_idx]\n",
    "\n",
    "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX_EN)\n",
    "    trg_padded = pad_sequence(trg_batch, batch_first=True, padding_value=PAD_IDX_FR)\n",
    "\n",
    "    return src_padded, trg_padded, src_lengths, trg_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6931cc20",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78b5b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(src_file, trg_file, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    dataset = TranslationDataset(src_file, trg_file, en_tokenizer, fr_tokenizer, en_vocab, fr_vocab)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec9729",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cfd6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, src, src_lengths):\n",
    "        # src: [batch, src_len]\n",
    "        embedded = self.embedding(src)  # [batch, src_len, embed_dim]\n",
    "        packed_emb = pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
    "        packed_outputs, (hidden, cell) = self.lstm(packed_emb)\n",
    "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True, padding_value=0.0)  # [batch, src_len, hidden_dim]\n",
    "        return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3875f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    \"\"\"Dot-product (Luong) attention. Computes attention weights and context.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs, mask=None):\n",
    "        # decoder_hidden: [batch, hidden_dim]\n",
    "        # encoder_outputs: [batch, src_len, hidden_dim]\n",
    "        # compute scores: batch bmm -> [batch, src_len]\n",
    "        scores = torch.bmm(encoder_outputs, decoder_hidden.unsqueeze(2)).squeeze(2)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(~mask, -1e9)\n",
    "        attn_weights = torch.softmax(scores, dim=1)  # [batch, src_len]\n",
    "        # context: [batch, hidden_dim]\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        return context, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe99b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        # combine decoder output and context vector\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.attention = LuongAttention()\n",
    "\n",
    "    def forward(self, input_token, hidden, cell, encoder_outputs, src_mask=None):\n",
    "        # input_token: [batch]\n",
    "        input_token = input_token.unsqueeze(1)            # [batch,1]\n",
    "        embedded = self.embedding(input_token)            # [batch,1,embed_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # output: [batch,1,hidden_dim]\n",
    "        dec_hidden = hidden[-1]  # [batch, hidden_dim]\n",
    "\n",
    "        # compute attention context\n",
    "        context, attn_weights = self.attention(dec_hidden, encoder_outputs, mask=src_mask)\n",
    "        # context: [batch, hidden_dim]\n",
    "\n",
    "        # concat output and context\n",
    "        concat = torch.cat([output.squeeze(1), context], dim=1)  # [batch, hidden_dim*2]\n",
    "        prediction = self.fc_out(concat)       # [batch, output_dim]\n",
    "        return prediction, hidden, cell, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f3db8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, teacher_forcing_ratio=TEACHER_FORCING_RATIO):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "    def forward(self, src, src_lengths, trg=None, teacher_forcing=True):\n",
    "        batch_size = src.size(0)\n",
    "        if trg is not None:\n",
    "            trg_len = trg.size(1)\n",
    "        else:\n",
    "            trg_len = MAX_LEN_DECODING\n",
    "\n",
    "        output_dim = self.decoder.fc_out.out_features\n",
    "        outputs = torch.zeros(batch_size, trg_len, output_dim).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)\n",
    "        # encoder_outputs: [batch, src_len, hidden_dim]\n",
    "        src_mask = (src != PAD_IDX_EN).to(self.device)  # [batch, src_len]\n",
    "\n",
    "        # first input = <sos>\n",
    "        if trg is not None:\n",
    "            input_token = trg[:, 0]\n",
    "        else:\n",
    "            sos_idx = fr_vocab['<sos>']\n",
    "            input_token = torch.tensor([sos_idx] * batch_size, dtype=torch.long, device=self.device)\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            if trg is not None and teacher_forcing:\n",
    "                teacher_force_flag = random.random() < self.teacher_forcing_ratio\n",
    "            else:\n",
    "                teacher_force_flag = False\n",
    "\n",
    "            output, hidden, cell, _ = self.decoder(input_token, hidden, cell, encoder_outputs, src_mask)\n",
    "            outputs[:, t] = output\n",
    "\n",
    "            top1 = output.argmax(1)\n",
    "            if teacher_force_flag and trg is not None:\n",
    "                input_token = trg[:, t]\n",
    "            else:\n",
    "                input_token = top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e5780",
   "metadata": {},
   "source": [
    "## Huấn luyện:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0ddb6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, clip, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, trg, src_lengths, trg_lengths in dataloader:\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        src_lengths = src_lengths.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, src_lengths, trg, teacher_forcing=True)\n",
    "        # output: [batch, trg_len, vocab_size]\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        # shift to ignore first token (<sos>)\n",
    "        output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "        trg_y = trg[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg_y)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98b15323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    if dataloader is None:\n",
    "        return float('inf')\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg, src_lengths, trg_lengths in dataloader:\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            src_lengths = src_lengths.to(device)\n",
    "\n",
    "            # disable teacher forcing\n",
    "            output = model(src, src_lengths, trg, teacher_forcing=False)\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "            trg_y = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg_y)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "033c1ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence: str, model, device, max_len=MAX_LEN_DECODING):\n",
    "    model.eval()\n",
    "    src_tensor = encode_sentence(sentence, en_tokenizer, en_vocab).unsqueeze(0).to(device)\n",
    "    src_lengths = torch.tensor([src_tensor.size(1)], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden, cell = model.encoder(src_tensor, src_lengths)\n",
    "\n",
    "    sos_idx = fr_vocab['<sos>']\n",
    "    eos_idx = fr_vocab['<eos>']\n",
    "    input_token = torch.tensor([sos_idx], dtype=torch.long, device=device)\n",
    "\n",
    "    src_mask = (src_tensor != PAD_IDX_EN).to(device)\n",
    "\n",
    "    output_ids = []\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            pred, hidden, cell, attn = model.decoder(input_token, hidden, cell, encoder_outputs, src_mask)\n",
    "        top1 = pred.argmax(1).item()\n",
    "        if top1 == eos_idx:\n",
    "            break\n",
    "        output_ids.append(top1)\n",
    "        input_token = torch.tensor([top1], dtype=torch.long, device=device)\n",
    "\n",
    "    # detokenize\n",
    "    try:\n",
    "        itos = fr_vocab.get_itos()\n",
    "    except:\n",
    "        itos = [tok for tok, idx in sorted(fr_vocab.get_stoi().items(), key=lambda x: x[1])]\n",
    "    words = [itos[i] for i in output_ids]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08b4c98",
   "metadata": {},
   "source": [
    "## Đánh giá BLEU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b50b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu(model, src_file, trg_file, device, max_samples=None):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    scores = []\n",
    "    with open(src_file, encoding='utf-8') as fsrc, open(trg_file, encoding='utf-8') as ftrg:\n",
    "        for i, (sline, tline) in enumerate(zip(fsrc, ftrg)):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "            sline = sline.strip()\n",
    "            tline = tline.strip()\n",
    "            pred = translate(sline, model, device)\n",
    "            reference = [tline.split()]\n",
    "            hypothesis = pred.split()\n",
    "            score = sentence_bleu(reference, hypothesis, smoothing_function=smoothie)\n",
    "            scores.append(score)\n",
    "    return sum(scores) / len(scores) if scores else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57656890",
   "metadata": {},
   "source": [
    "## Dịch thử:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97aa0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    enc = Encoder(len(en_vocab), EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT, PAD_IDX_EN)\n",
    "    dec = Decoder(len(fr_vocab), EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT, PAD_IDX_FR)\n",
    "    model = Seq2Seq(enc, dec, DEVICE, teacher_forcing_ratio=TEACHER_FORCING_RATIO).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX_FR)\n",
    "\n",
    "    train_loader = get_dataloader(EN_TRAIN, FR_TRAIN, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = get_dataloader(EN_VAL, FR_VAL, batch_size=BATCH_SIZE, shuffle=False) if os.path.exists(EN_VAL) else None\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    patience = 3\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        start = time.time()\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, CLIP, DEVICE)\n",
    "        valid_loss = evaluate(model, val_loader, criterion, DEVICE) if val_loader else train_loss\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        # === Lưu loss mỗi epoch ===\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {valid_loss:.4f} | Time: {end-start:.1f}s\")\n",
    "\n",
    "        # save checkpoint if improved\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'valid_loss': valid_loss\n",
    "            }, os.path.join(CHECKPOINT_DIR, 'attention_model.pth'))\n",
    "            wait = 0\n",
    "            print(\"  Saved new best model.\")\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # === Vẽ biểu đồ train/val loss ===\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(valid_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('Loss_curve_attention.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Đã lưu biểu đồ loss.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b05cbe9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best checkpoint.\n",
      "EN: A man in an orange hat starring at something.\n",
      "FR_pred: un homme en chapeau orange regarde quelque chose .\n",
      "------------------------------\n",
      "EN: A Boston Terrier is running on lush green grass in front of a white fence.\n",
      "FR_pred: un patineur est sur un herbe vert vert devant devant une clôture blanche .\n",
      "------------------------------\n",
      "EN: A girl in karate uniform breaking a stick with a front kick.\n",
      "FR_pred: une fille en tenue de karaté s' un bâton avec un coup de pied .\n",
      "------------------------------\n",
      "EN: Five people wearing winter jackets and helmets stand in the snow, with snowmobiles in the background.\n",
      "FR_pred: cinq personnes portant des manteaux d' hiver et des casques sont debout dans la neige , avec des buissons en arrière-plan .\n",
      "------------------------------\n",
      "EN: People are fixing the roof of a house.\n",
      "FR_pred: des gens réparent le moteur d' une maison .\n",
      "------------------------------\n",
      "BLEU on val: 0.2534060961687403\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # train (or you can load checkpoint)\n",
    "    # model = run_training()\n",
    "\n",
    "    # load best model if exists:\n",
    "    ckpt_path = os.path.join(CHECKPOINT_DIR, 'attention_model.pth')\n",
    "    if os.path.exists(ckpt_path):\n",
    "        data = torch.load(ckpt_path, map_location=DEVICE)\n",
    "        # must recreate model architecture then load\n",
    "        enc = Encoder(len(en_vocab), EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT, PAD_IDX_EN)\n",
    "        dec = Decoder(len(fr_vocab), EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT, PAD_IDX_FR)\n",
    "        model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
    "        model.load_state_dict(data['model_state_dict'])\n",
    "        print(\"Loaded best checkpoint.\")\n",
    "\n",
    "    TEST_FILE = './data/test_2016_flickr.en'\n",
    "    with open(TEST_FILE, encoding='utf-8') as f:\n",
    "        for i in range(5):\n",
    "            s = f.readline().strip()\n",
    "            print(\"EN:\", s)\n",
    "            print(\"FR_pred:\", translate(s, model, DEVICE))\n",
    "            print(\"-\"*30)\n",
    "\n",
    "    # compute BLEU on a small subset of validation if available\n",
    "    if os.path.exists(EN_VAL) and os.path.exists(FR_VAL):\n",
    "        bleu = compute_bleu(model, EN_VAL, FR_VAL, DEVICE, max_samples=200)\n",
    "        print(\"BLEU on val:\", bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3989735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to script main.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
