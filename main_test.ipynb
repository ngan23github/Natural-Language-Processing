{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c508c51",
   "metadata": {},
   "source": [
    "## Cài đặt môi trường"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2173cf",
   "metadata": {},
   "source": [
    "### Import thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25df0663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5038cd9f",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54e3fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 64\n",
    "EMBED_DIM = 512\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "N_EPOCHS = 20\n",
    "CLIP = 1.0\n",
    "TEACHER_FORCING_RATIO = 0.5\n",
    "MAX_LEN_DECODING = 50\n",
    "CHECKPOINT_DIR = './checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# adjust data paths if needed\n",
    "EN_TRAIN = './data/train.en'\n",
    "FR_TRAIN = './data/train.fr'\n",
    "EN_VAL = './data/val.en'\n",
    "FR_VAL = './data/val.fr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1cfa017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.1+cu117\n",
      "CUDA version: 11.7\n",
      "Is CUDA available: True\n",
      "GPU device name: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU device name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a09bb11",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05a71028",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f57ef",
   "metadata": {},
   "source": [
    "### Xây dựng từ điển"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af832272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(file_path, tokenizer):\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield tokenizer(line.strip().lower())\n",
    "\n",
    "def build_vocab(file_path, tokenizer, max_tokens=10000):\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        yield_tokens(file_path, tokenizer),\n",
    "        specials=['<unk>', '<pad>', '<sos>', '<eos>'],\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    return vocab\n",
    "\n",
    "en_vocab = build_vocab(EN_TRAIN, en_tokenizer)\n",
    "fr_vocab = build_vocab(FR_TRAIN, fr_tokenizer)\n",
    "\n",
    "PAD_IDX_EN = en_vocab['<pad>']\n",
    "PAD_IDX_FR = fr_vocab['<pad>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f644f8f",
   "metadata": {},
   "source": [
    "### Encode sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e404ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentence, tokenizer, vocab):\n",
    "    tokens = tokenizer(sentence.strip().lower())\n",
    "    ids = [vocab['<sos>']] + [vocab[token] for token in tokens] + [vocab['<eos>']]\n",
    "    return torch.tensor(ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e288010",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7efb9044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_file, trg_file, src_tokenizer, trg_tokenizer, src_vocab, trg_vocab):\n",
    "        with open(src_file, encoding='utf-8') as f:\n",
    "            self.src_lines = [l.strip() for l in f.readlines()]\n",
    "        with open(trg_file, encoding='utf-8') as f:\n",
    "            self.trg_lines = [l.strip() for l in f.readlines()]\n",
    "        assert len(self.src_lines) == len(self.trg_lines)\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = encode_sentence(self.src_lines[idx], self.src_tokenizer, self.src_vocab)\n",
    "        trg = encode_sentence(self.trg_lines[idx], self.trg_tokenizer, self.trg_vocab)\n",
    "        return src, trg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef32fa9",
   "metadata": {},
   "source": [
    "### Collate fn với Padding & Packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6cb6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_lengths = torch.tensor([len(s) for s in src_batch], dtype=torch.long)\n",
    "    trg_lengths = torch.tensor([len(t) for t in trg_batch], dtype=torch.long)\n",
    "\n",
    "    sorted_idx = torch.argsort(src_lengths, descending=True)\n",
    "    src_batch = [src_batch[i] for i in sorted_idx]\n",
    "    trg_batch = [trg_batch[i] for i in sorted_idx]\n",
    "    src_lengths = src_lengths[sorted_idx]\n",
    "    trg_lengths = trg_lengths[sorted_idx]\n",
    "\n",
    "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX_EN)\n",
    "    trg_padded = pad_sequence(trg_batch, batch_first=True, padding_value=PAD_IDX_FR)\n",
    "\n",
    "    return src_padded, trg_padded, src_lengths, trg_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6931cc20",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78b5b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(src_file, trg_file, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    dataset = TranslationDataset(src_file, trg_file, en_tokenizer, fr_tokenizer, en_vocab, fr_vocab)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec9729",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cfd6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, src, src_lengths):\n",
    "        # src: [batch, src_len]\n",
    "        embedded = self.embedding(src)  # [batch, src_len, embed_dim]\n",
    "        packed_emb = pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
    "        packed_outputs, (hidden, cell) = self.lstm(packed_emb)\n",
    "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True, padding_value=0.0)  # [batch, src_len, hidden_dim]\n",
    "        return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3875f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, decoder_hidden: torch.Tensor, encoder_outputs: torch.Tensor, mask: torch.Tensor = None):\n",
    "        # transform decoder hidden\n",
    "        dec_trans = self.W(decoder_hidden)  # [batch, hidden_dim]\n",
    "        # compute scores by dot product with encoder outputs\n",
    "        scores = torch.bmm(encoder_outputs, dec_trans.unsqueeze(2)).squeeze(2)  # [batch, src_len]\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(~mask, -1e9)\n",
    "        attn_weights = torch.softmax(scores, dim=1)  # [batch, src_len]\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)  # [batch, hidden_dim]\n",
    "        return context, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe99b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.attention = LuongAttention(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_token, hidden, cell, encoder_outputs, src_mask=None):\n",
    "        # input_token: [batch] (idx)\n",
    "        input_token = input_token.unsqueeze(1)  # [batch, 1]\n",
    "        embedded = self.dropout(self.embedding(input_token))  # [batch,1,embed_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))  # output: [batch,1,hidden_dim]\n",
    "        dec_hidden = hidden[-1]  # [batch, hidden_dim]\n",
    "        context, attn_weights = self.attention(dec_hidden, encoder_outputs, mask=src_mask)\n",
    "        concat = torch.cat([output.squeeze(1), context], dim=1)  # [batch, hidden_dim*2]\n",
    "        prediction = self.fc_out(concat)  # [batch, output_dim]\n",
    "        return prediction, hidden, cell, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f3db8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, teacher_forcing_ratio=TEACHER_FORCING_RATIO):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "    def forward(self, src, src_lengths, trg=None, teacher_forcing=True):\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1) if trg is not None else MAX_LEN_DECODING\n",
    "        output_dim = self.decoder.fc_out.out_features\n",
    "        outputs = torch.zeros(batch_size, trg_len, output_dim).to(self.device)\n",
    "        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)\n",
    "        src_mask = (src != PAD_IDX_EN).to(self.device)\n",
    "        input_token = trg[:,0] if trg is not None else torch.tensor([fr_vocab['<sos>']]*batch_size, device=self.device)\n",
    "        for t in range(1, trg_len):\n",
    "            teacher_force_flag = (random.random() < self.teacher_forcing_ratio) if (trg is not None and teacher_forcing) else False\n",
    "            output, hidden, cell, _ = self.decoder(input_token, hidden, cell, encoder_outputs, src_mask)\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input_token = trg[:,t] if teacher_force_flag and trg is not None else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e5780",
   "metadata": {},
   "source": [
    "## Huấn luyện:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0ddb6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, clip, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, trg, src_lengths, trg_lengths in dataloader:\n",
    "        src, trg, src_lengths = src.to(device), trg.to(device), src_lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, src_lengths, trg, teacher_forcing=True)\n",
    "        output_dim = output.shape[-1]\n",
    "        output_flat = output[:,1:,:].reshape(-1, output_dim)\n",
    "        trg_flat = trg[:,1:].reshape(-1)\n",
    "        loss = criterion(output_flat, trg_flat)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98b15323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    if dataloader is None:\n",
    "        return float('inf')\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg, src_lengths, trg_lengths in dataloader:\n",
    "            src, trg, src_lengths = src.to(device), trg.to(device), src_lengths.to(device)\n",
    "            output = model(src, src_lengths, trg, teacher_forcing=False)\n",
    "            output_dim = output.shape[-1]\n",
    "            output_flat = output[:,1:,:].reshape(-1, output_dim)\n",
    "            trg_flat = trg[:,1:].reshape(-1)\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=PAD_IDX_FR)(output_flat, trg_flat)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "033c1ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence: str, model, device, max_len=MAX_LEN_DECODING):\n",
    "    model.eval()\n",
    "    src_tensor = encode_sentence(sentence, en_tokenizer, en_vocab).unsqueeze(0).to(device)\n",
    "    src_lengths = torch.tensor([src_tensor.size(1)], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden, cell = model.encoder(src_tensor, src_lengths)\n",
    "\n",
    "    sos_idx = fr_vocab['<sos>']\n",
    "    eos_idx = fr_vocab['<eos>']\n",
    "    input_token = torch.tensor([sos_idx], dtype=torch.long, device=device)\n",
    "\n",
    "    src_mask = (src_tensor != PAD_IDX_EN).to(device)\n",
    "\n",
    "    output_ids = []\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            pred, hidden, cell, attn = model.decoder(input_token, hidden, cell, encoder_outputs, src_mask)\n",
    "        top1 = pred.argmax(1).item()\n",
    "        if top1 == eos_idx:\n",
    "            break\n",
    "        output_ids.append(top1)\n",
    "        input_token = torch.tensor([top1], dtype=torch.long, device=device)\n",
    "\n",
    "    # detokenize\n",
    "    try:\n",
    "        itos = fr_vocab.get_itos()\n",
    "    except:\n",
    "        itos = [tok for tok, idx in sorted(fr_vocab.get_stoi().items(), key=lambda x: x[1])]\n",
    "    words = [itos[i] for i in output_ids]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820da2b",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "189378ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_translate(sentence: str, model, device, max_len=MAX_LEN_DECODING, beam_width=5, alpha=0.7):\n",
    "    model.eval()\n",
    "    src_tensor = encode_sentence(sentence, en_tokenizer, en_vocab).unsqueeze(0).to(device)\n",
    "    src_lengths = torch.tensor([src_tensor.size(1)], device=device)\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden, cell = model.encoder(src_tensor, src_lengths)\n",
    "\n",
    "    sos_idx, eos_idx = fr_vocab['<sos>'], fr_vocab['<eos>']\n",
    "    src_mask = (src_tensor != PAD_IDX_EN).to(device)\n",
    "\n",
    "    # beam: list of (tokens, hidden, cell, score)\n",
    "    beam = [( [sos_idx], hidden, cell, 0.0 )]\n",
    "    completed = []\n",
    "\n",
    "    for _step in range(max_len):\n",
    "        new_beam = []\n",
    "        for tokens, h, c, score in beam:\n",
    "            if tokens[-1] == eos_idx:\n",
    "                completed.append((tokens, score))\n",
    "                continue\n",
    "            input_token = torch.tensor([tokens[-1]], device=device)\n",
    "            with torch.no_grad():\n",
    "                pred, h_new, c_new, _ = model.decoder(input_token, h, c, encoder_outputs, src_mask)\n",
    "                log_probs = F.log_softmax(pred, dim=1)  # [1, V]\n",
    "            topk_logp, topk_idx = torch.topk(log_probs, beam_width, dim=1)\n",
    "            for i in range(beam_width):\n",
    "                idx_i = topk_idx[0, i].item()\n",
    "                logp_i = topk_logp[0, i].item()\n",
    "                new_beam.append((tokens + [idx_i], h_new, c_new, score + logp_i))\n",
    "        # keep top-K by raw score\n",
    "        beam = sorted(new_beam, key=lambda x: x[3], reverse=True)[:beam_width]\n",
    "        if not beam:\n",
    "            break\n",
    "\n",
    "    # include leftover beams\n",
    "    for tokens, h, c, score in beam:\n",
    "        completed.append((tokens, score))\n",
    "\n",
    "    # normalize with length penalty and pick best\n",
    "    def norm_score(entry):\n",
    "        tokens, s = entry\n",
    "        length = len(tokens) - 1  # exclude <sos>\n",
    "        lp = ((5 + length) / 6) ** alpha\n",
    "        return s / lp\n",
    "\n",
    "    best_tokens, _ = max(completed, key=norm_score)\n",
    "    output_ids = best_tokens[1:]  # drop <sos>\n",
    "    if eos_idx in output_ids:\n",
    "        output_ids = output_ids[:output_ids.index(eos_idx)]\n",
    "    words = [fr_vocab.get_itos()[i] for i in output_ids if i < len(fr_vocab.get_itos())]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08b4c98",
   "metadata": {},
   "source": [
    "## Đánh giá BLEU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b50b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu(model, src_file, trg_file, device, max_samples=None, use_beam=False, beam_width=3):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    scores = []\n",
    "    with open(src_file, encoding='utf-8') as fsrc, open(trg_file, encoding='utf-8') as ftrg:\n",
    "        for i, (sline, tline) in enumerate(zip(fsrc, ftrg)):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "            sline = sline.strip()\n",
    "            tline = tline.strip()\n",
    "            if use_beam:\n",
    "                pred = beam_translate(sline, model, device, beam_width=beam_width)\n",
    "            else:\n",
    "                pred = translate(sline, model, device)\n",
    "            reference = [tline.split()]\n",
    "            hypothesis = pred.split()\n",
    "            score = sentence_bleu(reference, hypothesis, smoothing_function=smoothie)\n",
    "            scores.append(score)\n",
    "    return sum(scores)/len(scores) if scores else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57656890",
   "metadata": {},
   "source": [
    "## Dịch thử:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97aa0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    enc = Encoder(len(en_vocab), EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT, PAD_IDX_EN)\n",
    "    dec = Decoder(len(fr_vocab), EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT, PAD_IDX_FR)\n",
    "    model = Seq2Seq(enc, dec, DEVICE, TEACHER_FORCING_RATIO).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX_FR)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "    train_loader = get_dataloader(EN_TRAIN, FR_TRAIN, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = get_dataloader(EN_VAL, FR_VAL, batch_size=BATCH_SIZE, shuffle=False) if os.path.exists(EN_VAL) else None\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    patience = 3\n",
    "    wait = 0\n",
    "\n",
    "    # === Thêm biến lưu loss ===\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(1, N_EPOCHS+1):\n",
    "        start = time.time()\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, CLIP, DEVICE)\n",
    "        valid_loss = evaluate(model, val_loader, criterion, DEVICE) if val_loader else train_loss\n",
    "        end = time.time()\n",
    "\n",
    "        # === Lưu loss mỗi epoch ===\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {valid_loss:.4f} | Time: {end-start:.1f}s\")\n",
    "\n",
    "        # Save best checkpoint\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'valid_loss': valid_loss\n",
    "            }, os.path.join(CHECKPOINT_DIR, 'best_model.pth'))\n",
    "            wait = 0\n",
    "            print(\"  Saved new best model.\")\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "    # === Vẽ biểu đồ train/val loss ===\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(valid_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('Loss_curve.png', dpi=300)\n",
    "    plt.close()\n",
    "    print(\"Đã lưu biểu đồ loss tại file: Loss_curve.png\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b05cbe9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 4.3588 | Val Loss: 4.1540 | Time: 119.6s\n",
      "  Saved new best model.\n",
      "Epoch 2 | Train Loss: 3.0682 | Val Loss: 3.6620 | Time: 122.0s\n",
      "  Saved new best model.\n",
      "Epoch 3 | Train Loss: 2.4853 | Val Loss: 3.3680 | Time: 122.8s\n",
      "  Saved new best model.\n",
      "Epoch 4 | Train Loss: 2.0993 | Val Loss: 3.2631 | Time: 124.0s\n",
      "  Saved new best model.\n",
      "Epoch 5 | Train Loss: 1.8399 | Val Loss: 3.1422 | Time: 125.7s\n",
      "  Saved new best model.\n",
      "Epoch 6 | Train Loss: 1.6224 | Val Loss: 3.1857 | Time: 135.5s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     ckpt_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CHECKPOINT_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(ckpt_path):\n",
      "Cell \u001b[1;32mIn[23], line 24\u001b[0m, in \u001b[0;36mrun_training\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, N_EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     22\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 24\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, DEVICE) \u001b[38;5;28;01mif\u001b[39;00m val_loader \u001b[38;5;28;01melse\u001b[39;00m train_loss\n\u001b[0;32m     26\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataloader, optimizer, criterion, clip, device)\u001b[0m\n\u001b[0;32m     13\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n\u001b[0;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 15\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = run_training()\n",
    "    ckpt_path = os.path.join(CHECKPOINT_DIR, 'best_model.pth')\n",
    "    if os.path.exists(ckpt_path):\n",
    "        data = torch.load(ckpt_path, map_location=DEVICE)\n",
    "        enc = Encoder(len(en_vocab), EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT, PAD_IDX_EN)\n",
    "        dec = Decoder(len(fr_vocab), EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT, PAD_IDX_FR)\n",
    "        model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
    "        model.load_state_dict(data['model_state_dict'])\n",
    "        print(\"Loaded best checkpoint.\")\n",
    "\n",
    "    TEST_FILE = './data/test_2016_flickr.en'\n",
    "    with open(TEST_FILE, encoding='utf-8') as f:\n",
    "        for i in range(5):\n",
    "            s = f.readline().strip()\n",
    "            print(\"EN:\", s)\n",
    "            print(\"FR_pred:\", beam_translate(s, model, DEVICE, beam_width=3))\n",
    "            print(\"-\"*30)\n",
    "\n",
    "    if os.path.exists(EN_VAL) and os.path.exists(FR_VAL):\n",
    "        bleu = compute_bleu(model, EN_VAL, FR_VAL, DEVICE, max_samples=200, use_beam=True, beam_width=3)\n",
    "        print(\"BLEU on val:\", bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f2cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook main_beam_search.ipynb to script\n",
      "[NbConvertApp] Writing 16885 bytes to main_beam_search.py\n"
     ]
    }
   ],
   "source": [
    "# !jupyter nbconvert --to script main_beam_search.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
